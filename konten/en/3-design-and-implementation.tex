% Change the following title and label as desired.
\section{Design and Implementation}
\label{sec:designandimplementation}

\subsection{System Description}
\label{sec:systemdescription}

At this stage, the autonomous wheelchair is developed to follow humans using the YOLOv11-based detection algorithm. The system is designed with the primary goal of enhancing the mobility of users who need assistance in moving. The system consists of hardware and software components, integrated to achieve this goal.

\vspace{5pt}
\subsubsection{System Components}
\label{subsubsec:systemcomponents}

The system consists of:

\begin{itemize}
    \item \textbf{VS Code}: Used as a development environment to run YOLOv11-related code and other analyses.
    \item \textbf{Arduino IDE}: Used to develop and upload code to the ESP32 that controls the motors and sensors.
    \item \textbf{Laptop}: Serves as the main data processing center for more complex tasks and software development.
    \item \textbf{Camera (OV5640 5MP)}: Captures real-time images of the environment and detects targets (humans) using YOLOv11.
    \item \textbf{ESP32 Devkit V1}: Acts as the main controller that manages data from the camera and controls the wheelchair's movement.
    \item \textbf{2 Motor Controllers}: Control the DC motors that drive the wheelchair.
    \item \textbf{2 DC-DC Voltage Regulators}: Regulate the voltage for electronic components to remain stable.
    \item \textbf{2 DC Motors}: Drive the wheelchair, controlled through motor drivers that receive signals from the microcontroller.
    \item \textbf{24V Battery}: Serves as the main power source for the entire system, including the microcontroller, motors, and other devices.
\end{itemize}

\vspace{5pt}
\subsubsection{System Architecture}
\label{subsubsec:systemarchitecture}

The system architecture is designed so that the camera continuously captures images, then sends the data to the ESP32 microcontroller for processing. YOLOv11 is used to detect humans and provide coordinate information of the target's position. This information is then used by the microcontroller to control the motors and direct the wheelchair to dynamically follow the target's movements.

\subsection{Hardware}
\label{subsec:hardware}

The hardware design involves the integration of several components mentioned earlier. The camera is mounted at the front of the wheelchair to obtain an optimal view of the target. The ESP32 microcontroller is placed at the bottom of the wheelchair along with the motor drivers and battery to maintain balance.

\begin{itemize}
    \item \textbf{Control Unit}: A control unit such as a computer or laptop is used as the main processing center to run YOLOv11 and MediaPipe, analyze detection results, and generate decisions in the form of instruction codes.
    \item \textbf{OV5640 Camera}: This camera is connected to the ESP32 to capture frames using a 5MP sensor.
    \item \textbf{ESP32}: This microcontroller receives data from the camera and then runs the algorithm to be processed on the computer, then sends control signals to the motor driver.
    \item \textbf{L298N Motor Driver}: This driver is used to control the speed and direction of the DC motors that drive the wheelchair.
\end{itemize}

\vspace{5pt}
\subsubsection{Camera}
\label{subsubsec:camera}

The camera used in this system is the OV5640, a CMOS (Complementary Metal-Oxide-Semiconductor) camera sensor with a resolution of 5 megapixels (MP). This sensor can capture images up to a maximum resolution of 2592x1944 pixels, providing highly detailed image results. The sensor supports real-time image and video capture, with a frame rate of up to 30 frames per second (fps) at 1080p resolution, making it ideal for applications requiring direct image processing.

In addition to its high resolution, the OV5640 is also equipped with various advanced features for automatic image processing. According to the datasheet, features such as auto white balance, auto exposure, and auto focus allow this camera to automatically adapt to changes in lighting conditions and distance, thus consistently producing high-quality images in various situations. The OV5640 also supports face detection and image scaling features, which are very useful for quickly and accurately detecting objects or human targets.

\vspace{5pt}
\subsubsection{Control Unit}
\label{subsubsec:controlunit}

The control unit in this system acts as the main data processor during testing, with specifications designed to handle high computational loads and suitable for real-time data processing.

One important aspect of the control unit in this project is the support for fast and stable WiFi communication between devices. The system relies on video data sent by the OV5640 camera through the ESP32 to the laptop for processing, and this process must occur without interruption. The provided WiFi connectivity also allows real-time data transmission with minimal latency, which is crucial for quick target detection.

The ability to maintain stable WiFi connections at greater distances from the router is also important for testing in large areas. Handling multiple devices connected simultaneously allows efficient communication between ESP32 units without performance degradation. This technology is essential to ensure that the system can continuously process video data sent by the camera and provide instructions with a quick response.

The image data obtained using the OV5640 camera will be processed through a series of image data processing steps using a pre-trained detection model. This model is trained using the YOLOv11 architecture. The model used is named Best.pt, which is the result of training to detect the Human class.

To implement the system properly, several libraries need to be installed first. These libraries include OpenCV for image processing and Ultralytics for YOLO.

\begin{lstlisting}
  pip install OpenCV
  pip install Ultralytics
  ...etc
\end{lstlisting}

\vspace{5pt}
\subsubsection{ESP32}
\label{subsubsec:ESP32}

The ESP32 will receive directional data from the human detection classification results in the form of character letters such as A, B, C, D, or E. Based on previous research, the process of receiving string data by the ESP32 from connected devices is crucial for the smooth operation of the system.

Once the data is received and stored, the string is extracted into information about direction and speed. This extraction stage is crucial to convert the string into a format that can be used by the program to control the wheelchair motors. The extracted data is then displayed to ensure that the received direction and speed match the expected values. With the direction and speed information ready, the ESP32 can send instructions to the wheelchair motors to move according to the received data. This process will continue as long as the device remains connected and data continues to flow.

To provide a clearer understanding of the instructions received from the human detection classification results, the following table presents the instruction codes used in this program:

\begin{table}[H]
\centering
\caption{Instruction Codes from Classification Results}
\begin{tabular}{|c|c|}
\hline
\textbf{Pose Classification} & \textbf{Instruction Code} \\
\hline
Left & A \\
\hline
Forward & B \\
\hline
Stop & C \\
\hline
Backward & D \\
\hline
Right & E \\
\hline
\end{tabular}
\end{table}

These instruction codes are used to direct the wheelchair motors according to the detections made by the YOLOv11 classification model. For example, if the detected direction is "Left", the instruction code 'A' will be sent to move the wheelchair to the left. Similarly, if the detected direction is "Forward", the instruction 'B' will be sent to move the wheelchair forward. The code 'C' is used to stop the wheelchair when "Stop" is detected, 'D' to move backward when "Backward" is detected, and 'E' to move to the right when "Right" is detected.

This program ensures that the ESP32 functions effectively as a server receiving data from the NUC and using it to control the wheelchair motors. With the steps described, this program is designed to run continuously without interruption, waiting for and processing data sent by connected devices.
